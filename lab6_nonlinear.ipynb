{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cristian-Cortez/CSE337-labs/blob/main/lab6_nonlinear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 6: Non-linear function approximation\n",
        "\n",
        "## Exercise 1: Q-Learning with a Neural Network (PyTorch) on MountainCar\n",
        "\n",
        "**Objective:**\n",
        "Implement **Q-learning** with a **PyTorch neural network** to solve `MountainCar-v0`. You will approximate Q(s, a) with a small MLP, train it from batches of transitions sampled from a replay buffer, and evaluate the learned policy.\n",
        "\n",
        "---\n",
        "\n",
        "## Environment\n",
        "- **Gym** environment: `MountainCar-v0`\n",
        "- **State**: continuous (position, velocity) → shape `(2,)`\n",
        "- **Actions**: {0: left, 1: no push, 2: right}\n",
        "- **Reward**: -1 per step until the goal (`position >= 0.5`)\n",
        "- **Episode limit**: 500 steps\n",
        "- **Goal**: reduce steps-to-goal and improve return over training\n",
        "\n",
        "---\n",
        "\n",
        "## What You Must Implement\n",
        "\n",
        "### 1) Q-Network (PyTorch)\n",
        "Create a small MLP `QNetwork` that maps `state -> Q-values for 3 actions`.\n",
        "- Inputs: `(batch_size, 2)` float32\n",
        "- Outputs: `(batch_size, 3)` Q-values\n",
        "- Suggested architecture: `2 → 64 → 3` with ReLU\n",
        "- Initialize weights reasonably (PyTorch defaults are fine)\n",
        "\n",
        "### 2) Replay Buffer\n",
        "A cyclic buffer to store transitions `(s, a, r, s_next, done)`:\n",
        "- `append(s, a, r, s_next, done)`\n",
        "- `sample(batch_size)` → tensors ready for PyTorch (float32 for states, int64 for actions, float32 for rewards/done)\n",
        "\n",
        "### 3) ε-Greedy Policy\n",
        "- With probability `epsilon`: pick a random action\n",
        "- Otherwise: `argmax_a Q(s, a)` from the current network\n",
        "- Use **decaying ε** (e.g., from 1.0 down to 0.05 over ~20–50k steps)\n",
        "\n",
        "### 4) Q-Learning Target and Loss\n",
        "For a sampled batch:\n",
        "- Compute `q_pred = Q(s).gather(1, a)`  (shape `(batch, 1)`)\n",
        "- Compute target:\n",
        "  - If `done`: `target = r`\n",
        "  - Else: `target = r + gamma * max_a' Q(s_next, a').detach()`\n",
        "- Loss: Mean Squared Error (MSE) between `q_pred` and `target`\n",
        "\n",
        "> **Stabilization (recommended)**: Use a **target network** `Q_target` (periodically copy weights from `Q_online`) to compute the max over next-state actions. Update every `target_update_freq` steps.\n",
        "\n",
        "### 5) Deep Q-learning method\n",
        "- For each environment step:\n",
        "  1. Select action with ε-greedy\n",
        "  2. Step the env, store transition in buffer\n",
        "  3. If `len(buffer) >= batch_size`:\n",
        "     - Sample a batch\n",
        "     - Compute `q_pred`, `target`\n",
        "     - Backprop: `optimizer.zero_grad(); loss.backward(); optimizer.step()`\n",
        "     - (Optional) gradient clipping (e.g., `clip_grad_norm_` at 10)\n",
        "  4. Periodically update `Q_target ← Q_online` (if using target net)\n",
        "- Track episode returns (sum of rewards) and steps-to-goal\n",
        "\n",
        "---\n",
        "\n",
        "## Evaluation\n",
        "- Run **evaluation episodes** with `epsilon = 0.0` (greedy) every N training episodes\n",
        "- Report:\n",
        "  - Average steps-to-goal (lower is better; random policy is ~200)\n",
        "  - Average return (less negative is better)\n",
        "- Plot:\n",
        "  - Training episode return\n",
        "\n",
        "---\n",
        "\n",
        "## Deliverables\n",
        "1. **Code**: In a notebook.\n",
        "2. **Plots**:\n",
        "   - Episode  vs return\n",
        "   - Final value function (State (postition and velocity) Vs Max(Q(state)))\n",
        "\n",
        "3. **Short write-up** (also in the notebook):\n",
        "   - **Performance of your DQN agent**: How quickly does it learn? Does it reach the goal consistently?\n",
        "   - **Comparison with tile coding**:\n",
        "     - Which representation learns faster?\n",
        "     - Which one is more stable?\n",
        "     - How do the function approximation choices (linear with tiles vs. neural network) affect generalization?\n",
        "     - Did the NN require more tuning (learning rate, ε schedule) compared to tile coding?\n",
        "   - **Insights**: What are the trade-offs between hand-crafted features (tiles) and learned features (neural networks)?\n",
        "\n"
      ],
      "metadata": {
        "id": "DzEu8zQt3_MJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DHYJhmAv355q"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set up environment\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.99\n",
        "alpha = 0.001\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "num_episodes = 5000\n",
        "batch_size = 64\n",
        "replay_buffer_size = 50000\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Q-Network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, n_actions):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "JhoDESZd60Yu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Q-network and optimizer\n",
        "q_net = QNetwork(state_dim, n_actions).to(device)\n",
        "optimizer = optim.Adam(q_net.parameters(), lr=alpha)\n",
        "loss_fn = nn.MSELoss()\n",
        "replay_buffer = deque(maxlen=replay_buffer_size)"
      ],
      "metadata": {
        "id": "erbbkUXL65HM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy(state, epsilon):\n",
        "  ############ TODO ###########\n",
        "  # Convert state to torch tensor\n",
        "    state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "\n",
        "    # Exploration: pick a random action\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(0, n_actions)\n",
        "\n",
        "    # Exploitation: pick best action based on Q-network\n",
        "    with torch.no_grad():\n",
        "        q_values = q_net(state)\n",
        "        action = torch.argmax(q_values).item()  # Convert tensor → Python int\n",
        "    return action"
      ],
      "metadata": {
        "id": "nk6hESNp7KMk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dqn():\n",
        "    \"\"\"Train the DQN using experience replay.\"\"\"\n",
        "    if len(replay_buffer) < batch_size:\n",
        "        return\n",
        "    batch = random.sample(replay_buffer, batch_size)\n",
        "    states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "    states = torch.FloatTensor(states).to(device)\n",
        "    actions = torch.LongTensor(actions).to(device)\n",
        "    rewards = torch.FloatTensor(rewards).to(device)\n",
        "    next_states = torch.FloatTensor(next_states).to(device)\n",
        "    dones = torch.FloatTensor(dones).to(device)\n",
        "\n",
        "    q_values = q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "    next_q_values = q_net(next_states).max(1)[0].detach()\n",
        "    targets = rewards + gamma * next_q_values * (1 - dones)\n",
        "\n",
        "    loss = loss_fn(q_values, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "hcX56dEL7PZd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## MAIN Loop ###\n",
        "rewards_dqn = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "  state = env.reset()[0]\n",
        "  total_reward = 0\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "    action = epsilon_greedy(state, epsilon)\n",
        "    next_state, reward, done, _, _ = env.step(action)\n",
        "    replay_buffer.append((state, action, reward, next_state, done))\n",
        "    train_dqn()\n",
        "    state = next_state\n",
        "    total_reward += reward\n",
        "    ############ TODO ###########\n",
        "\n",
        "  # Epsilon decay after each episode\n",
        "  epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "  rewards_dqn.append(total_reward)\n",
        "\n",
        "  if (episode + 1) % 20 == 0:\n",
        "      avg_reward = np.mean(rewards_dqn[-100:])\n",
        "      print(f\"Episode {episode + 1}, Average Reward: {avg_reward:.2f}, Epsilon: {epsilon:.3f}\")"
      ],
      "metadata": {
        "id": "kgR0ojdN7RuM",
        "outputId": "6268c9de-5e7a-4067-e432-c42bdf1d5c84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 20, Average Reward: -7287.80, Epsilon: 0.887\n",
            "Episode 40, Average Reward: -4233.98, Epsilon: 0.802\n",
            "Episode 60, Average Reward: -3049.28, Epsilon: 0.726\n",
            "Episode 80, Average Reward: -2398.68, Epsilon: 0.656\n",
            "Episode 100, Average Reward: -1993.79, Epsilon: 0.594\n",
            "Episode 120, Average Reward: -612.81, Epsilon: 0.537\n",
            "Episode 140, Average Reward: -427.40, Epsilon: 0.486\n",
            "Episode 160, Average Reward: -338.13, Epsilon: 0.440\n",
            "Episode 180, Average Reward: -288.24, Epsilon: 0.398\n",
            "Episode 200, Average Reward: -254.47, Epsilon: 0.360\n",
            "Episode 220, Average Reward: -215.60, Epsilon: 0.325\n",
            "Episode 240, Average Reward: -204.14, Epsilon: 0.294\n",
            "Episode 260, Average Reward: -194.02, Epsilon: 0.266\n",
            "Episode 280, Average Reward: -189.32, Epsilon: 0.241\n",
            "Episode 300, Average Reward: -188.36, Epsilon: 0.218\n",
            "Episode 320, Average Reward: -185.42, Epsilon: 0.197\n",
            "Episode 340, Average Reward: -178.72, Epsilon: 0.178\n",
            "Episode 360, Average Reward: -172.27, Epsilon: 0.161\n",
            "Episode 380, Average Reward: -171.07, Epsilon: 0.146\n",
            "Episode 400, Average Reward: -163.49, Epsilon: 0.132\n",
            "Episode 420, Average Reward: -161.08, Epsilon: 0.119\n",
            "Episode 440, Average Reward: -160.73, Epsilon: 0.108\n",
            "Episode 460, Average Reward: -164.45, Epsilon: 0.098\n",
            "Episode 480, Average Reward: -169.26, Epsilon: 0.088\n",
            "Episode 500, Average Reward: -169.38, Epsilon: 0.080\n",
            "Episode 520, Average Reward: -169.57, Epsilon: 0.072\n",
            "Episode 540, Average Reward: -172.47, Epsilon: 0.065\n",
            "Episode 560, Average Reward: -172.96, Epsilon: 0.059\n",
            "Episode 580, Average Reward: -179.48, Epsilon: 0.054\n",
            "Episode 600, Average Reward: -176.48, Epsilon: 0.048\n",
            "Episode 620, Average Reward: -177.98, Epsilon: 0.044\n",
            "Episode 640, Average Reward: -175.33, Epsilon: 0.040\n",
            "Episode 660, Average Reward: -174.06, Epsilon: 0.036\n",
            "Episode 680, Average Reward: -160.09, Epsilon: 0.032\n",
            "Episode 700, Average Reward: -160.26, Epsilon: 0.029\n",
            "Episode 720, Average Reward: -154.85, Epsilon: 0.027\n",
            "Episode 740, Average Reward: -156.89, Epsilon: 0.024\n",
            "Episode 760, Average Reward: -153.36, Epsilon: 0.022\n",
            "Episode 780, Average Reward: -155.94, Epsilon: 0.020\n",
            "Episode 800, Average Reward: -159.25, Epsilon: 0.018\n",
            "Episode 820, Average Reward: -166.03, Epsilon: 0.016\n",
            "Episode 840, Average Reward: -165.46, Epsilon: 0.015\n",
            "Episode 860, Average Reward: -166.48, Epsilon: 0.013\n",
            "Episode 880, Average Reward: -164.65, Epsilon: 0.012\n",
            "Episode 900, Average Reward: -175.50, Epsilon: 0.011\n",
            "Episode 920, Average Reward: -165.79, Epsilon: 0.010\n",
            "Episode 940, Average Reward: -158.39, Epsilon: 0.010\n",
            "Episode 960, Average Reward: -155.35, Epsilon: 0.010\n",
            "Episode 980, Average Reward: -149.98, Epsilon: 0.010\n",
            "Episode 1000, Average Reward: -134.22, Epsilon: 0.010\n",
            "Episode 1020, Average Reward: -134.64, Epsilon: 0.010\n",
            "Episode 1040, Average Reward: -134.06, Epsilon: 0.010\n",
            "Episode 1060, Average Reward: -135.88, Epsilon: 0.010\n",
            "Episode 1080, Average Reward: -134.90, Epsilon: 0.010\n",
            "Episode 1100, Average Reward: -130.49, Epsilon: 0.010\n",
            "Episode 1120, Average Reward: -129.06, Epsilon: 0.010\n",
            "Episode 1140, Average Reward: -128.81, Epsilon: 0.010\n",
            "Episode 1160, Average Reward: -129.68, Epsilon: 0.010\n",
            "Episode 1180, Average Reward: -139.09, Epsilon: 0.010\n",
            "Episode 1200, Average Reward: -144.15, Epsilon: 0.010\n",
            "Episode 1220, Average Reward: -146.90, Epsilon: 0.010\n",
            "Episode 1240, Average Reward: -150.66, Epsilon: 0.010\n",
            "Episode 1260, Average Reward: -148.26, Epsilon: 0.010\n",
            "Episode 1280, Average Reward: -140.01, Epsilon: 0.010\n",
            "Episode 1300, Average Reward: -138.66, Epsilon: 0.010\n",
            "Episode 1320, Average Reward: -139.44, Epsilon: 0.010\n",
            "Episode 1340, Average Reward: -138.04, Epsilon: 0.010\n",
            "Episode 1360, Average Reward: -135.88, Epsilon: 0.010\n",
            "Episode 1380, Average Reward: -137.35, Epsilon: 0.010\n",
            "Episode 1400, Average Reward: -137.55, Epsilon: 0.010\n",
            "Episode 1420, Average Reward: -137.15, Epsilon: 0.010\n",
            "Episode 1440, Average Reward: -139.53, Epsilon: 0.010\n",
            "Episode 1460, Average Reward: -149.86, Epsilon: 0.010\n",
            "Episode 1480, Average Reward: -159.39, Epsilon: 0.010\n",
            "Episode 1500, Average Reward: -160.49, Epsilon: 0.010\n",
            "Episode 1520, Average Reward: -166.40, Epsilon: 0.010\n",
            "Episode 1540, Average Reward: -160.96, Epsilon: 0.010\n",
            "Episode 1560, Average Reward: -151.26, Epsilon: 0.010\n",
            "Episode 1580, Average Reward: -140.54, Epsilon: 0.010\n",
            "Episode 1600, Average Reward: -135.49, Epsilon: 0.010\n",
            "Episode 1620, Average Reward: -126.91, Epsilon: 0.010\n",
            "Episode 1640, Average Reward: -126.01, Epsilon: 0.010\n",
            "Episode 1660, Average Reward: -121.44, Epsilon: 0.010\n",
            "Episode 1680, Average Reward: -122.14, Epsilon: 0.010\n",
            "Episode 1700, Average Reward: -127.11, Epsilon: 0.010\n",
            "Episode 1720, Average Reward: -128.01, Epsilon: 0.010\n",
            "Episode 1740, Average Reward: -134.49, Epsilon: 0.010\n",
            "Episode 1760, Average Reward: -141.55, Epsilon: 0.010\n",
            "Episode 1780, Average Reward: -141.32, Epsilon: 0.010\n",
            "Episode 1800, Average Reward: -140.66, Epsilon: 0.010\n",
            "Episode 1820, Average Reward: -142.46, Epsilon: 0.010\n",
            "Episode 1840, Average Reward: -137.99, Epsilon: 0.010\n",
            "Episode 1860, Average Reward: -135.30, Epsilon: 0.010\n",
            "Episode 1880, Average Reward: -135.41, Epsilon: 0.010\n",
            "Episode 1900, Average Reward: -131.17, Epsilon: 0.010\n",
            "Episode 1920, Average Reward: -130.47, Epsilon: 0.010\n",
            "Episode 1940, Average Reward: -128.54, Epsilon: 0.010\n",
            "Episode 1960, Average Reward: -124.06, Epsilon: 0.010\n",
            "Episode 1980, Average Reward: -118.72, Epsilon: 0.010\n",
            "Episode 2000, Average Reward: -122.15, Epsilon: 0.010\n",
            "Episode 2020, Average Reward: -122.15, Epsilon: 0.010\n",
            "Episode 2040, Average Reward: -124.67, Epsilon: 0.010\n",
            "Episode 2060, Average Reward: -130.31, Epsilon: 0.010\n",
            "Episode 2080, Average Reward: -133.80, Epsilon: 0.010\n",
            "Episode 2100, Average Reward: -129.88, Epsilon: 0.010\n",
            "Episode 2120, Average Reward: -133.83, Epsilon: 0.010\n",
            "Episode 2140, Average Reward: -133.54, Epsilon: 0.010\n",
            "Episode 2160, Average Reward: -131.29, Epsilon: 0.010\n",
            "Episode 2180, Average Reward: -133.29, Epsilon: 0.010\n",
            "Episode 2200, Average Reward: -135.04, Epsilon: 0.010\n",
            "Episode 2220, Average Reward: -132.48, Epsilon: 0.010\n",
            "Episode 2240, Average Reward: -133.56, Epsilon: 0.010\n",
            "Episode 2260, Average Reward: -133.59, Epsilon: 0.010\n",
            "Episode 2280, Average Reward: -134.09, Epsilon: 0.010\n",
            "Episode 2300, Average Reward: -141.60, Epsilon: 0.010\n",
            "Episode 2320, Average Reward: -150.65, Epsilon: 0.010\n",
            "Episode 2340, Average Reward: -167.38, Epsilon: 0.010\n",
            "Episode 2360, Average Reward: -167.21, Epsilon: 0.010\n",
            "Episode 2380, Average Reward: -163.15, Epsilon: 0.010\n",
            "Episode 2400, Average Reward: -157.71, Epsilon: 0.010\n",
            "Episode 2420, Average Reward: -143.96, Epsilon: 0.010\n",
            "Episode 2440, Average Reward: -124.69, Epsilon: 0.010\n",
            "Episode 2460, Average Reward: -127.02, Epsilon: 0.010\n",
            "Episode 2480, Average Reward: -134.76, Epsilon: 0.010\n",
            "Episode 2500, Average Reward: -136.64, Epsilon: 0.010\n",
            "Episode 2520, Average Reward: -148.95, Epsilon: 0.010\n",
            "Episode 2540, Average Reward: -161.71, Epsilon: 0.010\n",
            "Episode 2560, Average Reward: -162.37, Epsilon: 0.010\n",
            "Episode 2580, Average Reward: -160.29, Epsilon: 0.010\n",
            "Episode 2600, Average Reward: -156.31, Epsilon: 0.010\n",
            "Episode 2620, Average Reward: -147.25, Epsilon: 0.010\n",
            "Episode 2640, Average Reward: -133.52, Epsilon: 0.010\n",
            "Episode 2660, Average Reward: -129.52, Epsilon: 0.010\n",
            "Episode 2680, Average Reward: -125.88, Epsilon: 0.010\n",
            "Episode 2700, Average Reward: -142.76, Epsilon: 0.010\n",
            "Episode 2720, Average Reward: -169.16, Epsilon: 0.010\n",
            "Episode 2740, Average Reward: -182.80, Epsilon: 0.010\n",
            "Episode 2760, Average Reward: -189.99, Epsilon: 0.010\n",
            "Episode 2780, Average Reward: -194.69, Epsilon: 0.010\n",
            "Episode 2800, Average Reward: -186.88, Epsilon: 0.010\n",
            "Episode 2820, Average Reward: -163.20, Epsilon: 0.010\n",
            "Episode 2840, Average Reward: -158.41, Epsilon: 0.010\n",
            "Episode 2860, Average Reward: -153.67, Epsilon: 0.010\n",
            "Episode 2880, Average Reward: -151.13, Epsilon: 0.010\n",
            "Episode 2900, Average Reward: -144.41, Epsilon: 0.010\n",
            "Episode 2920, Average Reward: -145.92, Epsilon: 0.010\n",
            "Episode 2940, Average Reward: -141.88, Epsilon: 0.010\n",
            "Episode 2960, Average Reward: -145.45, Epsilon: 0.010\n",
            "Episode 2980, Average Reward: -146.06, Epsilon: 0.010\n",
            "Episode 3000, Average Reward: -146.49, Epsilon: 0.010\n",
            "Episode 3020, Average Reward: -144.31, Epsilon: 0.010\n",
            "Episode 3040, Average Reward: -147.23, Epsilon: 0.010\n",
            "Episode 3060, Average Reward: -149.04, Epsilon: 0.010\n",
            "Episode 3080, Average Reward: -147.18, Epsilon: 0.010\n",
            "Episode 3100, Average Reward: -146.34, Epsilon: 0.010\n",
            "Episode 3120, Average Reward: -149.72, Epsilon: 0.010\n",
            "Episode 3140, Average Reward: -147.65, Epsilon: 0.010\n",
            "Episode 3160, Average Reward: -149.22, Epsilon: 0.010\n",
            "Episode 3180, Average Reward: -150.79, Epsilon: 0.010\n",
            "Episode 3200, Average Reward: -155.68, Epsilon: 0.010\n",
            "Episode 3220, Average Reward: -153.46, Epsilon: 0.010\n",
            "Episode 3240, Average Reward: -173.17, Epsilon: 0.010\n",
            "Episode 3260, Average Reward: -176.24, Epsilon: 0.010\n",
            "Episode 3280, Average Reward: -181.99, Epsilon: 0.010\n",
            "Episode 3300, Average Reward: -182.27, Epsilon: 0.010\n",
            "Episode 3320, Average Reward: -182.42, Epsilon: 0.010\n",
            "Episode 3340, Average Reward: -167.04, Epsilon: 0.010\n",
            "Episode 3360, Average Reward: -159.78, Epsilon: 0.010\n",
            "Episode 3380, Average Reward: -154.14, Epsilon: 0.010\n",
            "Episode 3400, Average Reward: -151.98, Epsilon: 0.010\n",
            "Episode 3420, Average Reward: -151.40, Epsilon: 0.010\n",
            "Episode 3440, Average Reward: -146.16, Epsilon: 0.010\n",
            "Episode 3460, Average Reward: -143.48, Epsilon: 0.010\n",
            "Episode 3480, Average Reward: -143.82, Epsilon: 0.010\n",
            "Episode 3500, Average Reward: -143.36, Epsilon: 0.010\n",
            "Episode 3520, Average Reward: -145.48, Epsilon: 0.010\n",
            "Episode 3540, Average Reward: -149.55, Epsilon: 0.010\n",
            "Episode 3560, Average Reward: -161.39, Epsilon: 0.010\n",
            "Episode 3580, Average Reward: -163.60, Epsilon: 0.010\n",
            "Episode 3600, Average Reward: -171.49, Epsilon: 0.010\n",
            "Episode 3620, Average Reward: -175.32, Epsilon: 0.010\n",
            "Episode 3640, Average Reward: -172.47, Epsilon: 0.010\n",
            "Episode 3660, Average Reward: -163.51, Epsilon: 0.010\n",
            "Episode 3680, Average Reward: -163.84, Epsilon: 0.010\n",
            "Episode 3700, Average Reward: -158.33, Epsilon: 0.010\n",
            "Episode 3720, Average Reward: -156.24, Epsilon: 0.010\n",
            "Episode 3740, Average Reward: -157.65, Epsilon: 0.010\n",
            "Episode 3760, Average Reward: -158.97, Epsilon: 0.010\n",
            "Episode 3780, Average Reward: -159.91, Epsilon: 0.010\n",
            "Episode 3800, Average Reward: -168.49, Epsilon: 0.010\n",
            "Episode 3820, Average Reward: -165.67, Epsilon: 0.010\n",
            "Episode 3840, Average Reward: -162.74, Epsilon: 0.010\n",
            "Episode 3860, Average Reward: -160.38, Epsilon: 0.010\n",
            "Episode 3880, Average Reward: -159.69, Epsilon: 0.010\n",
            "Episode 3900, Average Reward: -150.18, Epsilon: 0.010\n",
            "Episode 3920, Average Reward: -150.70, Epsilon: 0.010\n",
            "Episode 3940, Average Reward: -151.56, Epsilon: 0.010\n",
            "Episode 3960, Average Reward: -153.37, Epsilon: 0.010\n",
            "Episode 3980, Average Reward: -152.56, Epsilon: 0.010\n",
            "Episode 4000, Average Reward: -157.26, Epsilon: 0.010\n",
            "Episode 4020, Average Reward: -154.33, Epsilon: 0.010\n",
            "Episode 4040, Average Reward: -154.21, Epsilon: 0.010\n",
            "Episode 4060, Average Reward: -153.87, Epsilon: 0.010\n",
            "Episode 4080, Average Reward: -149.47, Epsilon: 0.010\n",
            "Episode 4100, Average Reward: -142.04, Epsilon: 0.010\n",
            "Episode 4120, Average Reward: -138.15, Epsilon: 0.010\n",
            "Episode 4140, Average Reward: -134.08, Epsilon: 0.010\n",
            "Episode 4160, Average Reward: -130.58, Epsilon: 0.010\n",
            "Episode 4180, Average Reward: -126.52, Epsilon: 0.010\n",
            "Episode 4200, Average Reward: -122.28, Epsilon: 0.010\n",
            "Episode 4220, Average Reward: -123.58, Epsilon: 0.010\n",
            "Episode 4240, Average Reward: -127.55, Epsilon: 0.010\n",
            "Episode 4260, Average Reward: -129.34, Epsilon: 0.010\n",
            "Episode 4280, Average Reward: -140.38, Epsilon: 0.010\n",
            "Episode 4300, Average Reward: -143.35, Epsilon: 0.010\n",
            "Episode 4320, Average Reward: -145.57, Epsilon: 0.010\n",
            "Episode 4340, Average Reward: -140.15, Epsilon: 0.010\n",
            "Episode 4360, Average Reward: -138.38, Epsilon: 0.010\n",
            "Episode 4380, Average Reward: -129.82, Epsilon: 0.010\n",
            "Episode 4400, Average Reward: -131.46, Epsilon: 0.010\n",
            "Episode 4420, Average Reward: -131.12, Epsilon: 0.010\n",
            "Episode 4440, Average Reward: -136.51, Epsilon: 0.010\n",
            "Episode 4460, Average Reward: -142.11, Epsilon: 0.010\n",
            "Episode 4480, Average Reward: -147.66, Epsilon: 0.010\n",
            "Episode 4500, Average Reward: -154.20, Epsilon: 0.010\n",
            "Episode 4520, Average Reward: -158.59, Epsilon: 0.010\n",
            "Episode 4540, Average Reward: -162.94, Epsilon: 0.010\n",
            "Episode 4560, Average Reward: -171.83, Epsilon: 0.010\n",
            "Episode 4580, Average Reward: -177.77, Epsilon: 0.010\n",
            "Episode 4600, Average Reward: -171.84, Epsilon: 0.010\n",
            "Episode 4620, Average Reward: -171.15, Epsilon: 0.010\n",
            "Episode 4640, Average Reward: -168.01, Epsilon: 0.010\n",
            "Episode 4660, Average Reward: -155.56, Epsilon: 0.010\n",
            "Episode 4680, Average Reward: -147.78, Epsilon: 0.010\n",
            "Episode 4700, Average Reward: -148.97, Epsilon: 0.010\n",
            "Episode 4720, Average Reward: -145.84, Epsilon: 0.010\n",
            "Episode 4740, Average Reward: -149.22, Epsilon: 0.010\n",
            "Episode 4760, Average Reward: -151.09, Epsilon: 0.010\n",
            "Episode 4780, Average Reward: -161.27, Epsilon: 0.010\n",
            "Episode 4800, Average Reward: -165.40, Epsilon: 0.010\n",
            "Episode 4820, Average Reward: -170.65, Epsilon: 0.010\n",
            "Episode 4840, Average Reward: -167.09, Epsilon: 0.010\n",
            "Episode 4860, Average Reward: -163.61, Epsilon: 0.010\n",
            "Episode 4880, Average Reward: -247.47, Epsilon: 0.010\n",
            "Episode 4900, Average Reward: -254.38, Epsilon: 0.010\n",
            "Episode 4920, Average Reward: -262.26, Epsilon: 0.010\n",
            "Episode 4940, Average Reward: -263.15, Epsilon: 0.010\n",
            "Episode 4960, Average Reward: -268.55, Epsilon: 0.010\n",
            "Episode 4980, Average Reward: -177.62, Epsilon: 0.010\n",
            "Episode 5000, Average Reward: -167.05, Epsilon: 0.010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance of your DQN agent: How quickly does it learn? Does it reach the goal consistently?\n",
        "\n",
        "*   The DQN agent steadily improves over the training. It takes about 260 episodes for the reward to consitently be around the -150 - -200 range, which is much better than the initial rewards which were in the negative thousands. After those first 260 episodes, it consistently reaches the goal.\n",
        "\n",
        "\n",
        "\n",
        "Comparison with tile coding:\n",
        "Which representation learns faster?\n",
        "Which one is more stable?\n",
        "\n",
        "\n",
        "*   I found that the tile coding learns faster and it more stable. It takes less episodes (around ~120) and it conistently stays to have less variance.\n",
        "\n",
        "\n",
        "\n",
        "How do the function approximation choices (linear with tiles vs. neural network) affect generalization?\n",
        "\n",
        "\n",
        "*    In my opinion, linear with tiles will have a more consistent and reliable generalization because they are more controlled (if multiple states activate the same tiles they will get similar predictions). For neural networks, however, they are less predictable generalizations because of its hidden layers. I am unsure if these layers can consistently recognize the patterns.\n",
        "\n",
        "\n",
        "Did the NN require more tuning (learning rate, ε schedule) compared to tile coding?\n",
        "Insights: What are the trade-offs between hand-crafted features (tiles) and learned features (neural networks)?\n",
        "\n",
        "\n",
        "*   No, the tile coding required much more tuning than the nerual network, which is a trade-off for its better predictibility and faster results. The nerual network, however, is much more convinent to set up, but there is less tuning you can do, since the pytorch library does most of the heavy lifting.\n",
        "\n"
      ],
      "metadata": {
        "id": "W3oks3I9jA6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2: Deep Q-Learning (DQN) on LunarLander-v2\n",
        "\n",
        "## Problem Description\n",
        "In this exercise, you will implement **Deep Q-Learning (DQN)** to solve the classic control problem **LunarLander-v2** in Gym.\n",
        "\n",
        "### The Task\n",
        "The agent controls a lander that starts at the top of the screen and must safely land on the landing pad between two flags.\n",
        "\n",
        "- **State space**: Continuous vector of 8 variables, including:\n",
        "  - Position (x, y)\n",
        "  - Velocity (x_dot, y_dot)\n",
        "  - Angle and angular velocity\n",
        "  - Left/right leg contact indicators\n",
        "- **Action space**: Discrete, 4 actions\n",
        "  - 0: do nothing\n",
        "  - 1: fire left orientation engine\n",
        "  - 2: fire main engine\n",
        "  - 3: fire right orientation engine\n",
        "- **Rewards**:\n",
        "  - +100 to +140 for successful landing\n",
        "  - -100 for crashing\n",
        "  - Small negative reward for firing engines (fuel cost)\n",
        "  - Episode ends when lander crashes or comes to rest\n",
        "\n",
        "The goal is to train an agent that lands successfully **most of the time**.\n",
        "\n",
        "---\n",
        "\n",
        "## Algorithm: Deep Q-Learning\n",
        "You will implement a **DQN agent** with the following components:\n",
        "\n",
        "1. **Q-Network**\n",
        "   - Neural network that approximates Q(s, a).\n",
        "   - Input: state vector (8 floats).\n",
        "   - Output: Q-values for 4 actions.\n",
        "   - Suggested architecture: 2 hidden layers with 128 neurons each, ReLU activation.\n",
        "\n",
        "2. **Target Network**\n",
        "   - A copy of the Q-network that is updated less frequently (e.g., every 1000 steps).\n",
        "   - Used for stable target computation.\n",
        "\n",
        "3. **Replay Buffer**\n",
        "   - Stores transitions `(s, a, r, s_next, done)`.\n",
        "   - Sample random mini-batches to break correlation between consecutive samples.\n",
        "\n",
        "4. **ε-Greedy Policy**\n",
        "   - With probability ε, take a random action.\n",
        "   - Otherwise, take `argmax_a Q(s, a)`.\n",
        "   - Decay ε over time (e.g., from 1.0 → 0.05).\n",
        "\n",
        "5. **Q-Learning Method**\n",
        "   \n",
        "\n",
        "\n",
        "**Final note:**\n",
        "   No code base is necessary. At this point, you must know how to implement evertything.\n",
        "   For reference, but not recommended ([Here](https://colab.research.google.com/drive/1Gl0kuln79A__hgf2a-_-mwoGISXQDK_X?authuser=1#scrollTo=8Sd0q9DG8Rt8&line=56&uniqifier=1) is a solution)\n",
        "\n",
        "---\n",
        "## Deliverables\n",
        "1. **Code**:\n",
        "- Q-network (PyTorch).\n",
        "- Training loop with ε-greedy policy, target network, and Adam optimizer.\n",
        "\n",
        "2. **Plots**:\n",
        "- Episode returns vs training episodes.\n",
        "- Evaluation performance with a greedy policy (ε = 0).\n",
        "\n",
        "3. **Short Write-up (≤1 page)**:\n",
        "- Did your agent learn to land consistently?  \n",
        "- How many episodes did it take before you saw improvement?  \n",
        "- What effect did replay buffer size, target update frequency, and learning rate have on stability?  \n",
        "- Compare results across different runs (does it sometimes fail to converge?).\n",
        "\n",
        "Compare this task with the **MountainCar-v0** problem you solved earlier:\n",
        "- What is **extra** or more challenging in LunarLander?  \n",
        "- Consider state dimensionality, number of actions, reward shaping, and the difficulty of exploration.  \n",
        "- Why might DQN be necessary here, whereas simpler methods (like tile coding) could work for MountainCar?\n"
      ],
      "metadata": {
        "id": "8Sd0q9DG8Rt8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J6IXyZqR7zia"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}